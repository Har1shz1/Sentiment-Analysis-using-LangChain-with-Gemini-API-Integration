{ 
  "cells": [ 
    {    
      "cell_type": "code",  
      "execution_count": null,   
      "metadata": { 
        "colab": {
          "base_uri": "https://localhost:8080/", 
          "height": 903
        },
        "id": "eyx1GUhQzDhL",
        "outputId": "a4aa131a-7cf5-4a63-bccf-1460171986e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m303.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-072bdbd245df>:23: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  @validator(\"sentiment\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6dfaf4762aa1fbeb36.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://6dfaf4762aa1fbeb36.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install -q --upgrade langchain-google-genai google-generativeai python-dotenv matplotlib pandas gradio\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "GOOGLE_API_KEY = \"YOUR_API_KEY_HERE\" #replace your api key here\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "class SentimentAnalysis(BaseModel):\n",
        "    sentiment: str = Field(description=\"POSITIVE, NEGATIVE, NEUTRAL, IRRELEVANT, HARMFUL, or SUGGESTIVE\")\n",
        "    emotion: str = Field(description=\"Dominant emotion such as Happy, Angry, Sad, Confused, etc.\")\n",
        "    rating: int = Field(ge=1, le=5, description=\"Rating between 1 (worst) to 5 (best)\")\n",
        "\n",
        "    @validator(\"sentiment\")\n",
        "    def sentiment_allowed(cls, value):\n",
        "        allowed = {\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\", \"IRRELEVANT\", \"HARMFUL\", \"SUGGESTIVE\"}\n",
        "        if value.upper() not in allowed:\n",
        "            raise ValueError(f\"Sentiment must be one of {allowed}\")\n",
        "        return value.upper()\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=SentimentAnalysis)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Analyze the user feedback below and respond with:\n",
        "1. Sentiment (POSITIVE, NEGATIVE, NEUTRAL, IRRELEVANT, HARMFUL, or SUGGESTIVE),\n",
        "2. Dominant emotion (e.g. Happy, Angry, Sad),\n",
        "3. Rating from 1 to 5.\n",
        "{format_instructions}\n",
        "Feedback: {query}\n",
        "\"\"\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "chain = prompt | model | parser\n",
        "\n",
        "def analyze_feedbacks(text_input, file):\n",
        "    feedbacks = []\n",
        "    if file is not None:\n",
        "        content = file.decode(\"utf-8\")\n",
        "        feedbacks = [line.strip() for line in content.strip().split(\"\\n\") if line.strip()]\n",
        "    elif text_input.strip():\n",
        "        feedbacks = [line.strip() for line in text_input.strip().split(\"\\n\") if line.strip()]\n",
        "\n",
        "    if not feedbacks:\n",
        "        return \"Please provide some feedback.\", None\n",
        "\n",
        "    results = []\n",
        "    for fb in feedbacks:\n",
        "        try:\n",
        "            result = chain.invoke({\"query\": fb})\n",
        "            results.append({\n",
        "                \"Feedback\": fb,\n",
        "                \"Sentiment\": result.sentiment,\n",
        "                \"Emotion\": result.emotion,\n",
        "                \"Rating\": result.rating\n",
        "            })\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                \"Feedback\": fb,\n",
        "                \"Sentiment\": \"Error\",\n",
        "                \"Emotion\": \"Error\",\n",
        "                \"Rating\": \"Error\"\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    sentiment_counts = df['Sentiment'].value_counts()\n",
        "    axs[0].pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140)\n",
        "    axs[0].set_title(\"Sentiment Distribution\")\n",
        "\n",
        "    try:\n",
        "        rating_counts = df['Rating'].value_counts().sort_index()\n",
        "        axs[1].bar(rating_counts.index.astype(str), rating_counts.values, color='skyblue')\n",
        "        axs[1].set_title(\"Rating Frequency\")\n",
        "        axs[1].set_xlabel(\"Rating\")\n",
        "        axs[1].set_ylabel(\"Count\")\n",
        "    except:\n",
        "        axs[1].text(0.5, 0.5, \"Ratings not valid\", ha='center', va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return df, fig\n",
        "\n",
        "with gr.Blocks(title=\"Feedback Sentiment Analyzer with Gemini\") as demo:\n",
        "    gr.Markdown(\"## Feedback Sentiment Analyzer using Gemini Flash\")\n",
        "    gr.Markdown(\"You can enter feedback manually (one per line) or upload a .txt file.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        text_box = gr.Textbox(label=\"Enter Feedback (One per Line)\", lines=8, placeholder=\"The teacher was very helpful...\")\n",
        "        file_upload = gr.File(label=\"Upload a .txt File\", file_types=[\".txt\"], type=\"binary\")\n",
        "\n",
        "    submit_btn = gr.Button(\"Analyze Feedback\")\n",
        "\n",
        "    output_table = gr.Dataframe(label=\"Results\")\n",
        "    output_plot = gr.Plot(label=\"Sentiment & Rating Charts\")\n",
        "\n",
        "    submit_btn.click(analyze_feedbacks, inputs=[text_box, file_upload], outputs=[output_table, output_plot])\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
